{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5URi95VNn23"
      },
      "source": [
        "# **Cancer Decision Tree Classification**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Import Packages**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "gwe9FeysidtE"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting Pyspark\n",
            "  Using cached pyspark-3.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: py4j==0.10.9.7 in c:\\python311\\lib\\site-packages (from Pyspark) (0.10.9.7)\n",
            "Installing collected packages: Pyspark\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Could not install packages due to an OSError: [Errno 13] Permission denied: 'C:\\\\Python311\\\\Scripts\\\\beeline'\n",
            "Consider using the `--user` option or check the permissions.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install Pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Start SPARK Session**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "J5wjW5Ud6McP"
      },
      "outputs": [],
      "source": [
        "\n",
        "from pyspark.sql import SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "CELhWhbn6McP"
      },
      "outputs": [],
      "source": [
        "spark=SparkSession.builder.getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "bj7IILDV6McP",
        "outputId": "c738d15a-d129-43a7-8381-b1aa481a2672"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://192.168.1.35:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x217efe8b910>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Read sklearn inbuilt data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "xcsHrgdw6glu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.4.0-1-cp311-cp311-win_amd64.whl (10.6 MB)\n",
            "                                              0.0/10.6 MB ? eta -:--:--\n",
            "     -                                        0.4/10.6 MB 11.2 MB/s eta 0:00:01\n",
            "     -                                        0.5/10.6 MB 6.6 MB/s eta 0:00:02\n",
            "     ---                                      0.9/10.6 MB 7.0 MB/s eta 0:00:02\n",
            "     ----                                     1.1/10.6 MB 6.5 MB/s eta 0:00:02\n",
            "     -----                                    1.4/10.6 MB 6.3 MB/s eta 0:00:02\n",
            "     ------                                   1.6/10.6 MB 6.4 MB/s eta 0:00:02\n",
            "     -------                                  1.9/10.6 MB 6.3 MB/s eta 0:00:02\n",
            "     --------                                 2.1/10.6 MB 6.2 MB/s eta 0:00:02\n",
            "     ---------                                2.4/10.6 MB 6.1 MB/s eta 0:00:02\n",
            "     ---------                                2.6/10.6 MB 6.0 MB/s eta 0:00:02\n",
            "     ----------                               2.9/10.6 MB 6.0 MB/s eta 0:00:02\n",
            "     -----------                              3.2/10.6 MB 5.9 MB/s eta 0:00:02\n",
            "     ------------                             3.4/10.6 MB 5.9 MB/s eta 0:00:02\n",
            "     -------------                            3.7/10.6 MB 5.9 MB/s eta 0:00:02\n",
            "     --------------                           3.9/10.6 MB 5.9 MB/s eta 0:00:02\n",
            "     ---------------                          4.2/10.6 MB 5.8 MB/s eta 0:00:02\n",
            "     ----------------                         4.5/10.6 MB 5.8 MB/s eta 0:00:02\n",
            "     -----------------                        4.7/10.6 MB 5.8 MB/s eta 0:00:02\n",
            "     ------------------                       5.0/10.6 MB 5.8 MB/s eta 0:00:01\n",
            "     -------------------                      5.2/10.6 MB 5.8 MB/s eta 0:00:01\n",
            "     --------------------                     5.5/10.6 MB 5.8 MB/s eta 0:00:01\n",
            "     ---------------------                    5.7/10.6 MB 5.8 MB/s eta 0:00:01\n",
            "     ----------------------                   6.0/10.6 MB 5.8 MB/s eta 0:00:01\n",
            "     -----------------------                  6.2/10.6 MB 5.8 MB/s eta 0:00:01\n",
            "     ------------------------                 6.5/10.6 MB 5.8 MB/s eta 0:00:01\n",
            "     -------------------------                6.7/10.6 MB 5.7 MB/s eta 0:00:01\n",
            "     --------------------------               7.0/10.6 MB 5.7 MB/s eta 0:00:01\n",
            "     ---------------------------              7.3/10.6 MB 5.7 MB/s eta 0:00:01\n",
            "     ----------------------------             7.5/10.6 MB 5.7 MB/s eta 0:00:01\n",
            "     -----------------------------            7.8/10.6 MB 5.7 MB/s eta 0:00:01\n",
            "     ------------------------------           8.0/10.6 MB 5.7 MB/s eta 0:00:01\n",
            "     -------------------------------          8.3/10.6 MB 5.7 MB/s eta 0:00:01\n",
            "     --------------------------------         8.6/10.6 MB 5.7 MB/s eta 0:00:01\n",
            "     ---------------------------------        8.8/10.6 MB 5.7 MB/s eta 0:00:01\n",
            "     ---------------------------------        8.9/10.6 MB 5.7 MB/s eta 0:00:01\n",
            "     ----------------------------------       9.3/10.6 MB 5.7 MB/s eta 0:00:01\n",
            "     ------------------------------------     9.6/10.6 MB 5.7 MB/s eta 0:00:01\n",
            "     -------------------------------------    9.8/10.6 MB 5.7 MB/s eta 0:00:01\n",
            "     -------------------------------------    10.1/10.6 MB 5.7 MB/s eta 0:00:01\n",
            "     --------------------------------------   10.3/10.6 MB 5.6 MB/s eta 0:00:01\n",
            "     ---------------------------------------  10.5/10.6 MB 5.5 MB/s eta 0:00:01\n",
            "     ---------------------------------------  10.6/10.6 MB 5.6 MB/s eta 0:00:01\n",
            "     ---------------------------------------- 10.6/10.6 MB 5.3 MB/s eta 0:00:00\n",
            "Requirement already satisfied: numpy<2.0,>=1.19.5 in c:\\python311\\lib\\site-packages (from scikit-learn) (1.24.2)\n",
            "Collecting scipy>=1.6.0 (from scikit-learn)\n",
            "  Downloading scipy-1.12.0-cp311-cp311-win_amd64.whl (46.2 MB)\n",
            "                                              0.0/46.2 MB ? eta -:--:--\n",
            "                                              0.4/46.2 MB 7.6 MB/s eta 0:00:07\n",
            "                                              0.5/46.2 MB 6.6 MB/s eta 0:00:07\n",
            "                                              0.7/46.2 MB 5.2 MB/s eta 0:00:09\n",
            "                                              0.9/46.2 MB 5.0 MB/s eta 0:00:10\n",
            "                                              1.1/46.2 MB 4.7 MB/s eta 0:00:10\n",
            "     -                                        1.4/46.2 MB 4.9 MB/s eta 0:00:10\n",
            "     -                                        1.6/46.2 MB 5.0 MB/s eta 0:00:09\n",
            "     -                                        1.9/46.2 MB 5.1 MB/s eta 0:00:09\n",
            "     -                                        2.2/46.2 MB 5.1 MB/s eta 0:00:09\n",
            "     --                                       2.4/46.2 MB 5.1 MB/s eta 0:00:09\n",
            "     --                                       2.7/46.2 MB 5.2 MB/s eta 0:00:09\n",
            "     --                                       2.9/46.2 MB 5.2 MB/s eta 0:00:09\n",
            "     --                                       3.2/46.2 MB 5.2 MB/s eta 0:00:09\n",
            "     --                                       3.4/46.2 MB 5.4 MB/s eta 0:00:08\n",
            "     ---                                      3.7/46.2 MB 5.3 MB/s eta 0:00:08\n",
            "     ---                                      3.9/46.2 MB 5.3 MB/s eta 0:00:08\n",
            "     ---                                      4.2/46.2 MB 5.4 MB/s eta 0:00:08\n",
            "     ---                                      4.5/46.2 MB 5.4 MB/s eta 0:00:08\n",
            "     ----                                     4.7/46.2 MB 5.4 MB/s eta 0:00:08\n",
            "     ----                                     5.0/46.2 MB 5.4 MB/s eta 0:00:08\n",
            "     ----                                     5.2/46.2 MB 5.4 MB/s eta 0:00:08\n",
            "     ----                                     5.5/46.2 MB 5.4 MB/s eta 0:00:08\n",
            "     ----                                     5.7/46.2 MB 5.4 MB/s eta 0:00:08\n",
            "     -----                                    6.0/46.2 MB 5.4 MB/s eta 0:00:08\n",
            "     -----                                    6.2/46.2 MB 5.5 MB/s eta 0:00:08\n",
            "     -----                                    6.5/46.2 MB 5.5 MB/s eta 0:00:08\n",
            "     -----                                    6.8/46.2 MB 5.5 MB/s eta 0:00:08\n",
            "     ------                                   7.0/46.2 MB 5.5 MB/s eta 0:00:08\n",
            "     ------                                   7.3/46.2 MB 5.5 MB/s eta 0:00:08\n",
            "     ------                                   7.5/46.2 MB 5.5 MB/s eta 0:00:08\n",
            "     ------                                   7.8/46.2 MB 5.5 MB/s eta 0:00:08\n",
            "     ------                                   7.9/46.2 MB 5.4 MB/s eta 0:00:08\n",
            "     ------                                   7.9/46.2 MB 5.4 MB/s eta 0:00:08\n",
            "     ------                                   8.0/46.2 MB 5.1 MB/s eta 0:00:08\n",
            "     -------                                  8.1/46.2 MB 5.1 MB/s eta 0:00:08\n",
            "     -------                                  8.3/46.2 MB 5.0 MB/s eta 0:00:08\n",
            "     -------                                  8.6/46.2 MB 5.0 MB/s eta 0:00:08\n",
            "     -------                                  8.9/46.2 MB 5.1 MB/s eta 0:00:08\n",
            "     -------                                  9.1/46.2 MB 5.1 MB/s eta 0:00:08\n",
            "     --------                                 9.4/46.2 MB 5.1 MB/s eta 0:00:08\n",
            "     --------                                 9.6/46.2 MB 5.1 MB/s eta 0:00:08\n",
            "     --------                                 9.9/46.2 MB 5.1 MB/s eta 0:00:08\n",
            "     --------                                 10.1/46.2 MB 5.1 MB/s eta 0:00:08\n",
            "     ---------                                10.4/46.2 MB 5.1 MB/s eta 0:00:07\n",
            "     ---------                                10.6/46.2 MB 5.1 MB/s eta 0:00:08\n",
            "     ---------                                10.9/46.2 MB 5.2 MB/s eta 0:00:07\n",
            "     ---------                                11.2/46.2 MB 5.2 MB/s eta 0:00:07\n",
            "     ---------                                11.4/46.2 MB 5.2 MB/s eta 0:00:07\n",
            "     ----------                               11.7/46.2 MB 5.2 MB/s eta 0:00:07\n",
            "     ----------                               11.9/46.2 MB 5.2 MB/s eta 0:00:07\n",
            "     ----------                               12.2/46.2 MB 5.2 MB/s eta 0:00:07\n",
            "     ----------                               12.4/46.2 MB 5.2 MB/s eta 0:00:07\n",
            "     ----------                               12.7/46.2 MB 5.2 MB/s eta 0:00:07\n",
            "     -----------                              12.9/46.2 MB 5.2 MB/s eta 0:00:07\n",
            "     -----------                              13.2/46.2 MB 5.2 MB/s eta 0:00:07\n",
            "     -----------                              13.4/46.2 MB 5.2 MB/s eta 0:00:07\n",
            "     -----------                              13.7/46.2 MB 5.2 MB/s eta 0:00:07\n",
            "     ------------                             14.0/46.2 MB 5.2 MB/s eta 0:00:07\n",
            "     ------------                             14.2/46.2 MB 5.2 MB/s eta 0:00:07\n",
            "     ------------                             14.4/46.2 MB 5.2 MB/s eta 0:00:07\n",
            "     ------------                             14.7/46.2 MB 5.2 MB/s eta 0:00:07\n",
            "     ------------                             15.0/46.2 MB 5.2 MB/s eta 0:00:07\n",
            "     -------------                            15.2/46.2 MB 5.2 MB/s eta 0:00:06\n",
            "     -------------                            15.5/46.2 MB 5.2 MB/s eta 0:00:06\n",
            "     -------------                            15.7/46.2 MB 5.2 MB/s eta 0:00:06\n",
            "     -------------                            16.0/46.2 MB 5.2 MB/s eta 0:00:06\n",
            "     --------------                           16.2/46.2 MB 5.2 MB/s eta 0:00:06\n",
            "     --------------                           16.2/46.2 MB 5.2 MB/s eta 0:00:06\n",
            "     --------------                           16.6/46.2 MB 5.2 MB/s eta 0:00:06\n",
            "     --------------                           16.8/46.2 MB 5.2 MB/s eta 0:00:06\n",
            "     --------------                           17.0/46.2 MB 5.1 MB/s eta 0:00:06\n",
            "     --------------                           17.3/46.2 MB 5.1 MB/s eta 0:00:06\n",
            "     ---------------                          17.6/46.2 MB 5.1 MB/s eta 0:00:06\n",
            "     ---------------                          17.8/46.2 MB 5.1 MB/s eta 0:00:06\n",
            "     ---------------                          18.1/46.2 MB 5.1 MB/s eta 0:00:06\n",
            "     ---------------                          18.3/46.2 MB 5.4 MB/s eta 0:00:06\n",
            "     ----------------                         18.6/46.2 MB 5.5 MB/s eta 0:00:06\n",
            "     ----------------                         18.8/46.2 MB 5.5 MB/s eta 0:00:06\n",
            "     ----------------                         19.1/46.2 MB 5.5 MB/s eta 0:00:05\n",
            "     ----------------                         19.3/46.2 MB 5.5 MB/s eta 0:00:05\n",
            "     ----------------                         19.6/46.2 MB 5.5 MB/s eta 0:00:05\n",
            "     -----------------                        19.9/46.2 MB 5.5 MB/s eta 0:00:05\n",
            "     -----------------                        20.1/46.2 MB 5.5 MB/s eta 0:00:05\n",
            "     -----------------                        20.4/46.2 MB 5.5 MB/s eta 0:00:05\n",
            "     -----------------                        20.6/46.2 MB 5.5 MB/s eta 0:00:05\n",
            "     ------------------                       20.9/46.2 MB 5.5 MB/s eta 0:00:05\n",
            "     ------------------                       21.1/46.2 MB 5.5 MB/s eta 0:00:05\n",
            "     ------------------                       21.4/46.2 MB 5.5 MB/s eta 0:00:05\n",
            "     ------------------                       21.6/46.2 MB 5.5 MB/s eta 0:00:05\n",
            "     ------------------                       21.9/46.2 MB 5.5 MB/s eta 0:00:05\n",
            "     -------------------                      22.1/46.2 MB 5.5 MB/s eta 0:00:05\n",
            "     -------------------                      22.4/46.2 MB 5.5 MB/s eta 0:00:05\n",
            "     -------------------                      22.7/46.2 MB 5.5 MB/s eta 0:00:05\n",
            "     -------------------                      22.9/46.2 MB 5.5 MB/s eta 0:00:05\n",
            "     --------------------                     23.2/46.2 MB 5.5 MB/s eta 0:00:05\n",
            "     --------------------                     23.4/46.2 MB 5.5 MB/s eta 0:00:05\n",
            "     --------------------                     23.7/46.2 MB 5.5 MB/s eta 0:00:05\n",
            "     --------------------                     23.9/46.2 MB 5.5 MB/s eta 0:00:05\n",
            "     --------------------                     24.2/46.2 MB 5.5 MB/s eta 0:00:05\n",
            "     ---------------------                    24.4/46.2 MB 5.5 MB/s eta 0:00:04\n",
            "     ---------------------                    24.7/46.2 MB 5.5 MB/s eta 0:00:04\n",
            "     ---------------------                    25.0/46.2 MB 5.5 MB/s eta 0:00:04\n",
            "     ---------------------                    25.2/46.2 MB 5.5 MB/s eta 0:00:04\n",
            "     ----------------------                   25.5/46.2 MB 5.5 MB/s eta 0:00:04\n",
            "     ----------------------                   25.7/46.2 MB 5.5 MB/s eta 0:00:04\n",
            "     ----------------------                   26.0/46.2 MB 5.5 MB/s eta 0:00:04\n",
            "     ----------------------                   26.2/46.2 MB 5.5 MB/s eta 0:00:04\n",
            "     ----------------------                   26.5/46.2 MB 5.5 MB/s eta 0:00:04\n",
            "     -----------------------                  26.8/46.2 MB 5.5 MB/s eta 0:00:04\n",
            "     -----------------------                  27.0/46.2 MB 5.5 MB/s eta 0:00:04\n",
            "     -----------------------                  27.3/46.2 MB 5.5 MB/s eta 0:00:04\n",
            "     -----------------------                  27.5/46.2 MB 5.6 MB/s eta 0:00:04\n",
            "     ------------------------                 27.8/46.2 MB 5.6 MB/s eta 0:00:04\n",
            "     ------------------------                 28.0/46.2 MB 5.6 MB/s eta 0:00:04\n",
            "     ------------------------                 28.3/46.2 MB 5.6 MB/s eta 0:00:04\n",
            "     ------------------------                 28.5/46.2 MB 5.6 MB/s eta 0:00:04\n",
            "     ------------------------                 28.8/46.2 MB 5.6 MB/s eta 0:00:04\n",
            "     -------------------------                29.1/46.2 MB 5.6 MB/s eta 0:00:04\n",
            "     -------------------------                29.3/46.2 MB 5.6 MB/s eta 0:00:04\n",
            "     -------------------------                29.6/46.2 MB 5.6 MB/s eta 0:00:03\n",
            "     -------------------------                29.8/46.2 MB 5.6 MB/s eta 0:00:03\n",
            "     --------------------------               30.1/46.2 MB 5.5 MB/s eta 0:00:03\n",
            "     --------------------------               30.4/46.2 MB 5.5 MB/s eta 0:00:03\n",
            "     --------------------------               30.6/46.2 MB 5.5 MB/s eta 0:00:03\n",
            "     --------------------------               30.9/46.2 MB 5.6 MB/s eta 0:00:03\n",
            "     --------------------------               31.1/46.2 MB 5.6 MB/s eta 0:00:03\n",
            "     --------------------------               31.1/46.2 MB 5.5 MB/s eta 0:00:03\n",
            "     ---------------------------              31.4/46.2 MB 5.5 MB/s eta 0:00:03\n",
            "     ---------------------------              31.6/46.2 MB 5.5 MB/s eta 0:00:03\n",
            "     ---------------------------              31.8/46.2 MB 5.4 MB/s eta 0:00:03\n",
            "     ---------------------------              31.8/46.2 MB 5.4 MB/s eta 0:00:03\n",
            "     ---------------------------              32.1/46.2 MB 5.3 MB/s eta 0:00:03\n",
            "     ----------------------------             32.4/46.2 MB 5.3 MB/s eta 0:00:03\n",
            "     ----------------------------             32.6/46.2 MB 5.3 MB/s eta 0:00:03\n",
            "     ----------------------------             32.8/46.2 MB 5.3 MB/s eta 0:00:03\n",
            "     ----------------------------             33.0/46.2 MB 5.2 MB/s eta 0:00:03\n",
            "     ----------------------------             33.2/46.2 MB 5.2 MB/s eta 0:00:03\n",
            "     ----------------------------             33.4/46.2 MB 5.2 MB/s eta 0:00:03\n",
            "     -----------------------------            33.7/46.2 MB 5.2 MB/s eta 0:00:03\n",
            "     -----------------------------            34.0/46.2 MB 5.2 MB/s eta 0:00:03\n",
            "     -----------------------------            34.2/46.2 MB 5.2 MB/s eta 0:00:03\n",
            "     -----------------------------            34.5/46.2 MB 5.2 MB/s eta 0:00:03\n",
            "     ------------------------------           34.7/46.2 MB 5.2 MB/s eta 0:00:03\n",
            "     ------------------------------           35.0/46.2 MB 5.2 MB/s eta 0:00:03\n",
            "     ------------------------------           35.2/46.2 MB 5.2 MB/s eta 0:00:03\n",
            "     ------------------------------           35.5/46.2 MB 5.2 MB/s eta 0:00:03\n",
            "     ------------------------------           35.7/46.2 MB 5.2 MB/s eta 0:00:03\n",
            "     -------------------------------          36.0/46.2 MB 5.2 MB/s eta 0:00:02\n",
            "     -------------------------------          36.2/46.2 MB 5.2 MB/s eta 0:00:02\n",
            "     -------------------------------          36.5/46.2 MB 5.2 MB/s eta 0:00:02\n",
            "     -------------------------------          36.8/46.2 MB 5.2 MB/s eta 0:00:02\n",
            "     --------------------------------         37.0/46.2 MB 5.2 MB/s eta 0:00:02\n",
            "     --------------------------------         37.3/46.2 MB 5.2 MB/s eta 0:00:02\n",
            "     --------------------------------         37.5/46.2 MB 5.2 MB/s eta 0:00:02\n",
            "     --------------------------------         37.8/46.2 MB 5.2 MB/s eta 0:00:02\n",
            "     --------------------------------         38.0/46.2 MB 5.2 MB/s eta 0:00:02\n",
            "     ---------------------------------        38.3/46.2 MB 5.2 MB/s eta 0:00:02\n",
            "     ---------------------------------        38.5/46.2 MB 5.2 MB/s eta 0:00:02\n",
            "     ---------------------------------        38.8/46.2 MB 5.2 MB/s eta 0:00:02\n",
            "     ---------------------------------        39.0/46.2 MB 5.2 MB/s eta 0:00:02\n",
            "     ----------------------------------       39.3/46.2 MB 5.2 MB/s eta 0:00:02\n",
            "     ----------------------------------       39.6/46.2 MB 5.2 MB/s eta 0:00:02\n",
            "     ----------------------------------       39.8/46.2 MB 5.2 MB/s eta 0:00:02\n",
            "     ----------------------------------       40.1/46.2 MB 5.2 MB/s eta 0:00:02\n",
            "     ----------------------------------       40.3/46.2 MB 5.2 MB/s eta 0:00:02\n",
            "     -----------------------------------      40.6/46.2 MB 5.2 MB/s eta 0:00:02\n",
            "     -----------------------------------      40.8/46.2 MB 5.2 MB/s eta 0:00:02\n",
            "     -----------------------------------      41.1/46.2 MB 5.2 MB/s eta 0:00:01\n",
            "     -----------------------------------      41.3/46.2 MB 5.2 MB/s eta 0:00:01\n",
            "     ------------------------------------     41.6/46.2 MB 5.3 MB/s eta 0:00:01\n",
            "     ------------------------------------     41.9/46.2 MB 5.3 MB/s eta 0:00:01\n",
            "     ------------------------------------     42.1/46.2 MB 5.5 MB/s eta 0:00:01\n",
            "     ------------------------------------     42.4/46.2 MB 5.5 MB/s eta 0:00:01\n",
            "     ------------------------------------     42.6/46.2 MB 5.5 MB/s eta 0:00:01\n",
            "     -------------------------------------    42.9/46.2 MB 5.5 MB/s eta 0:00:01\n",
            "     -------------------------------------    43.1/46.2 MB 5.5 MB/s eta 0:00:01\n",
            "     -------------------------------------    43.4/46.2 MB 5.6 MB/s eta 0:00:01\n",
            "     -------------------------------------    43.6/46.2 MB 5.6 MB/s eta 0:00:01\n",
            "     --------------------------------------   43.9/46.2 MB 5.6 MB/s eta 0:00:01\n",
            "     --------------------------------------   44.2/46.2 MB 5.6 MB/s eta 0:00:01\n",
            "     --------------------------------------   44.4/46.2 MB 5.6 MB/s eta 0:00:01\n",
            "     --------------------------------------   44.7/46.2 MB 5.6 MB/s eta 0:00:01\n",
            "     --------------------------------------   44.9/46.2 MB 5.6 MB/s eta 0:00:01\n",
            "     ---------------------------------------  45.2/46.2 MB 5.6 MB/s eta 0:00:01\n",
            "     ---------------------------------------  45.4/46.2 MB 5.6 MB/s eta 0:00:01\n",
            "     ---------------------------------------  45.7/46.2 MB 5.6 MB/s eta 0:00:01\n",
            "     ---------------------------------------  45.9/46.2 MB 5.6 MB/s eta 0:00:01\n",
            "     ---------------------------------------  46.2/46.2 MB 5.6 MB/s eta 0:00:01\n",
            "     ---------------------------------------  46.2/46.2 MB 5.6 MB/s eta 0:00:01\n",
            "     ---------------------------------------  46.2/46.2 MB 5.6 MB/s eta 0:00:01\n",
            "     ---------------------------------------  46.2/46.2 MB 5.6 MB/s eta 0:00:01\n",
            "     ---------------------------------------  46.2/46.2 MB 5.6 MB/s eta 0:00:01\n",
            "     ---------------------------------------  46.2/46.2 MB 5.6 MB/s eta 0:00:01\n",
            "     ---------------------------------------  46.2/46.2 MB 5.6 MB/s eta 0:00:01\n",
            "     ---------------------------------------  46.2/46.2 MB 5.6 MB/s eta 0:00:01\n",
            "     ---------------------------------------  46.2/46.2 MB 5.6 MB/s eta 0:00:01\n",
            "     ---------------------------------------- 46.2/46.2 MB 4.5 MB/s eta 0:00:00\n",
            "Collecting joblib>=1.2.0 (from scikit-learn)\n",
            "  Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
            "                                              0.0/302.2 kB ? eta -:--:--\n",
            "     -------------------------------------  297.0/302.2 kB 9.2 MB/s eta 0:00:01\n",
            "     -------------------------------------- 302.2/302.2 kB 9.4 MB/s eta 0:00:00\n",
            "Collecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
            "  Downloading threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
            "Successfully installed joblib-1.3.2 scikit-learn-1.4.0 scipy-1.12.0 threadpoolctl-3.2.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Python311\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:485: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
            "  if should_localize and is_datetime64tz_dtype(s.dtype) and s.dt.tz is not None:\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "!pip install scikit-learn\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "cancer = load_breast_cancer(as_frame=True)\n",
        "cancer = cancer.frame\n",
        "cancer = spark.createDataFrame(cancer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRXO_exO6glv",
        "outputId": "5f02880a-9b29-416f-d3ea-aaae476d4f85"
      },
      "outputs": [
        {
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o48.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (192.168.1.35 executor driver): java.io.IOException: Cannot run program \"python3\": CreateProcess error=2, The system cannot find the file specified\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1143)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1073)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:181)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: java.io.IOException: CreateProcess error=2, The system cannot find the file specified\r\n\tat java.base/java.lang.ProcessImpl.create(Native Method)\r\n\tat java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:494)\r\n\tat java.base/java.lang.ProcessImpl.start(ProcessImpl.java:159)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1110)\r\n\t... 34 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3326)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3326)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3549)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: java.io.IOException: Cannot run program \"python3\": CreateProcess error=2, The system cannot find the file specified\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1143)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1073)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:181)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\t... 1 more\r\nCaused by: java.io.IOException: CreateProcess error=2, The system cannot find the file specified\r\n\tat java.base/java.lang.ProcessImpl.create(Native Method)\r\n\tat java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:494)\r\n\tat java.base/java.lang.ProcessImpl.start(ProcessImpl.java:159)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1110)\r\n\t... 34 more\r\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[1;32md:\\nagav\\Downloads\\Cancer_Decision_Tree_Classification.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/nagav/Downloads/Cancer_Decision_Tree_Classification.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m cancer\u001b[39m.\u001b[39;49mshow()\n",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:959\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    953\u001b[0m     \u001b[39mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    954\u001b[0m         error_class\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNOT_BOOL\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    955\u001b[0m         message_parameters\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39marg_name\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mvertical\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39marg_type\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mtype\u001b[39m(vertical)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m},\n\u001b[0;32m    956\u001b[0m     )\n\u001b[0;32m    958\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(truncate, \u001b[39mbool\u001b[39m) \u001b[39mand\u001b[39;00m truncate:\n\u001b[1;32m--> 959\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mshowString(n, \u001b[39m20\u001b[39;49m, vertical))\n\u001b[0;32m    960\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    961\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
            "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o48.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (192.168.1.35 executor driver): java.io.IOException: Cannot run program \"python3\": CreateProcess error=2, The system cannot find the file specified\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1143)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1073)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:181)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: java.io.IOException: CreateProcess error=2, The system cannot find the file specified\r\n\tat java.base/java.lang.ProcessImpl.create(Native Method)\r\n\tat java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:494)\r\n\tat java.base/java.lang.ProcessImpl.start(ProcessImpl.java:159)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1110)\r\n\t... 34 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3326)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3326)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3549)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: java.io.IOException: Cannot run program \"python3\": CreateProcess error=2, The system cannot find the file specified\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1143)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1073)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:181)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\t... 1 more\r\nCaused by: java.io.IOException: CreateProcess error=2, The system cannot find the file specified\r\n\tat java.base/java.lang.ProcessImpl.create(Native Method)\r\n\tat java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:494)\r\n\tat java.base/java.lang.ProcessImpl.start(ProcessImpl.java:159)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1110)\r\n\t... 34 more\r\n"
          ]
        }
      ],
      "source": [
        "cancer.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFDhxcnB6glv",
        "outputId": "26f36e5a-63f1-41ef-cb49-08e0ae300f81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- mean radius: double (nullable = true)\n",
            " |-- mean texture: double (nullable = true)\n",
            " |-- mean perimeter: double (nullable = true)\n",
            " |-- mean area: double (nullable = true)\n",
            " |-- mean smoothness: double (nullable = true)\n",
            " |-- mean compactness: double (nullable = true)\n",
            " |-- mean concavity: double (nullable = true)\n",
            " |-- mean concave points: double (nullable = true)\n",
            " |-- mean symmetry: double (nullable = true)\n",
            " |-- mean fractal dimension: double (nullable = true)\n",
            " |-- radius error: double (nullable = true)\n",
            " |-- texture error: double (nullable = true)\n",
            " |-- perimeter error: double (nullable = true)\n",
            " |-- area error: double (nullable = true)\n",
            " |-- smoothness error: double (nullable = true)\n",
            " |-- compactness error: double (nullable = true)\n",
            " |-- concavity error: double (nullable = true)\n",
            " |-- concave points error: double (nullable = true)\n",
            " |-- symmetry error: double (nullable = true)\n",
            " |-- fractal dimension error: double (nullable = true)\n",
            " |-- worst radius: double (nullable = true)\n",
            " |-- worst texture: double (nullable = true)\n",
            " |-- worst perimeter: double (nullable = true)\n",
            " |-- worst area: double (nullable = true)\n",
            " |-- worst smoothness: double (nullable = true)\n",
            " |-- worst compactness: double (nullable = true)\n",
            " |-- worst concavity: double (nullable = true)\n",
            " |-- worst concave points: double (nullable = true)\n",
            " |-- worst symmetry: double (nullable = true)\n",
            " |-- worst fractal dimension: double (nullable = true)\n",
            " |-- target: long (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "cancer.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TorzEMY6glw",
        "outputId": "0b07af62-88be-4fd0-96fe-771711386bdb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['mean radius',\n",
              " 'mean texture',\n",
              " 'mean perimeter',\n",
              " 'mean area',\n",
              " 'mean smoothness',\n",
              " 'mean compactness',\n",
              " 'mean concavity',\n",
              " 'mean concave points',\n",
              " 'mean symmetry',\n",
              " 'mean fractal dimension',\n",
              " 'radius error',\n",
              " 'texture error',\n",
              " 'perimeter error',\n",
              " 'area error',\n",
              " 'smoothness error',\n",
              " 'compactness error',\n",
              " 'concavity error',\n",
              " 'concave points error',\n",
              " 'symmetry error',\n",
              " 'fractal dimension error',\n",
              " 'worst radius',\n",
              " 'worst texture',\n",
              " 'worst perimeter',\n",
              " 'worst area',\n",
              " 'worst smoothness',\n",
              " 'worst compactness',\n",
              " 'worst concavity',\n",
              " 'worst concave points',\n",
              " 'worst symmetry',\n",
              " 'worst fractal dimension',\n",
              " 'target']"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cancer.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "wZjuNOGL6glw"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import VectorAssembler "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "ooSm4UaF6glw"
      },
      "outputs": [],
      "source": [
        "featureassembler= VectorAssembler(inputCols=['mean radius',\n",
        " 'mean texture',\n",
        " 'mean perimeter',\n",
        " 'mean area',\n",
        " 'mean smoothness',\n",
        " 'mean compactness',\n",
        " 'mean concavity',\n",
        " 'mean concave points',\n",
        " 'mean symmetry',\n",
        " 'mean fractal dimension',\n",
        " 'radius error',\n",
        " 'texture error',\n",
        " 'perimeter error',\n",
        " 'area error',\n",
        " 'smoothness error',\n",
        " 'compactness error',\n",
        " 'concavity error',\n",
        " 'concave points error',\n",
        " 'symmetry error',\n",
        " 'fractal dimension error',\n",
        " 'worst radius',\n",
        " 'worst texture',\n",
        " 'worst perimeter',\n",
        " 'worst area',\n",
        " 'worst smoothness',\n",
        " 'worst compactness',\n",
        " 'worst concavity',\n",
        " 'worst concave points',\n",
        " 'worst symmetry',\n",
        " 'worst fractal dimension',], outputCol='Features')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "jJt4WucO6glw"
      },
      "outputs": [],
      "source": [
        "output = featureassembler.transform(cancer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jY_eaYl_6glw",
        "outputId": "72486c10-5dd4-4490-81b9-12ee9612dbf5"
      },
      "outputs": [
        {
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o68.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2) (192.168.1.35 executor driver): java.io.IOException: Cannot run program \"python3\": CreateProcess error=2, The system cannot find the file specified\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1143)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1073)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:181)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: java.io.IOException: CreateProcess error=2, The system cannot find the file specified\r\n\tat java.base/java.lang.ProcessImpl.create(Native Method)\r\n\tat java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:494)\r\n\tat java.base/java.lang.ProcessImpl.start(ProcessImpl.java:159)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1110)\r\n\t... 34 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3326)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3326)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3549)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: java.io.IOException: Cannot run program \"python3\": CreateProcess error=2, The system cannot find the file specified\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1143)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1073)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:181)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\t... 1 more\r\nCaused by: java.io.IOException: CreateProcess error=2, The system cannot find the file specified\r\n\tat java.base/java.lang.ProcessImpl.create(Native Method)\r\n\tat java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:494)\r\n\tat java.base/java.lang.ProcessImpl.start(ProcessImpl.java:159)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1110)\r\n\t... 34 more\r\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[1;32md:\\nagav\\Downloads\\Cancer_Decision_Tree_Classification.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/nagav/Downloads/Cancer_Decision_Tree_Classification.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m output\u001b[39m.\u001b[39;49mshow()\n",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:959\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    953\u001b[0m     \u001b[39mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    954\u001b[0m         error_class\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNOT_BOOL\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    955\u001b[0m         message_parameters\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39marg_name\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mvertical\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39marg_type\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mtype\u001b[39m(vertical)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m},\n\u001b[0;32m    956\u001b[0m     )\n\u001b[0;32m    958\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(truncate, \u001b[39mbool\u001b[39m) \u001b[39mand\u001b[39;00m truncate:\n\u001b[1;32m--> 959\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mshowString(n, \u001b[39m20\u001b[39;49m, vertical))\n\u001b[0;32m    960\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    961\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
            "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o68.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2) (192.168.1.35 executor driver): java.io.IOException: Cannot run program \"python3\": CreateProcess error=2, The system cannot find the file specified\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1143)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1073)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:181)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: java.io.IOException: CreateProcess error=2, The system cannot find the file specified\r\n\tat java.base/java.lang.ProcessImpl.create(Native Method)\r\n\tat java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:494)\r\n\tat java.base/java.lang.ProcessImpl.start(ProcessImpl.java:159)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1110)\r\n\t... 34 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3326)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3326)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3549)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: java.io.IOException: Cannot run program \"python3\": CreateProcess error=2, The system cannot find the file specified\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1143)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1073)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:181)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\t... 1 more\r\nCaused by: java.io.IOException: CreateProcess error=2, The system cannot find the file specified\r\n\tat java.base/java.lang.ProcessImpl.create(Native Method)\r\n\tat java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:494)\r\n\tat java.base/java.lang.ProcessImpl.start(ProcessImpl.java:159)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1110)\r\n\t... 34 more\r\n"
          ]
        }
      ],
      "source": [
        "output.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "08O1r4kX6glx"
      },
      "outputs": [],
      "source": [
        "modeldata=output.select('Features','target')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DyyYW9Ml6glx",
        "outputId": "45dd4d00-17fb-42fd-f008-00a26512768c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+------+\n",
            "|            Features|target|\n",
            "+--------------------+------+\n",
            "|[17.99,10.38,122....|     0|\n",
            "|[20.57,17.77,132....|     0|\n",
            "|[19.69,21.25,130....|     0|\n",
            "|[11.42,20.38,77.5...|     0|\n",
            "|[20.29,14.34,135....|     0|\n",
            "|[12.45,15.7,82.57...|     0|\n",
            "|[18.25,19.98,119....|     0|\n",
            "|[13.71,20.83,90.2...|     0|\n",
            "|[13.0,21.82,87.5,...|     0|\n",
            "|[12.46,24.04,83.9...|     0|\n",
            "|[16.02,23.24,102....|     0|\n",
            "|[15.78,17.89,103....|     0|\n",
            "|[19.17,24.8,132.4...|     0|\n",
            "|[15.85,23.95,103....|     0|\n",
            "|[13.73,22.61,93.6...|     0|\n",
            "|[14.54,27.54,96.7...|     0|\n",
            "|[14.68,20.13,94.7...|     0|\n",
            "|[16.13,20.68,108....|     0|\n",
            "|[19.81,22.15,130....|     0|\n",
            "|[13.54,14.36,87.4...|     1|\n",
            "+--------------------+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "modeldata.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Split data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "-KbpnDWC6glx"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_data,test_data=modeldata.randomSplit([0.8,0.2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZcfp4R26glx",
        "outputId": "fa51f37c-21ca-498e-e93f-b0f04e32ed45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+------+\n",
            "|            Features|target|\n",
            "+--------------------+------+\n",
            "|[6.981,13.43,43.7...|     1|\n",
            "|[8.196,16.84,51.7...|     1|\n",
            "|[8.219,20.7,53.27...|     1|\n",
            "|[8.598,20.98,54.6...|     1|\n",
            "|[8.618,11.79,54.3...|     1|\n",
            "|[8.888,14.64,58.7...|     1|\n",
            "|[8.95,15.76,58.74...|     1|\n",
            "|[9.029,17.33,58.7...|     1|\n",
            "|[9.173,13.86,59.2...|     1|\n",
            "|[9.504,12.44,60.3...|     1|\n",
            "|[9.567,15.91,60.2...|     1|\n",
            "|[9.72,18.22,60.73...|     1|\n",
            "|[9.731,15.34,63.7...|     1|\n",
            "|[9.738,11.97,61.2...|     1|\n",
            "|[9.742,15.67,61.5...|     1|\n",
            "|[9.777,16.99,62.5...|     1|\n",
            "|[9.787,19.94,62.1...|     1|\n",
            "|[9.876,17.27,62.9...|     1|\n",
            "|[9.904,18.06,64.6...|     1|\n",
            "|[10.08,15.11,63.7...|     1|\n",
            "+--------------------+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "train_data.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Decision Tree Classification Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "u-RUxJVW6glx"
      },
      "outputs": [],
      "source": [
        "\n",
        "from pyspark.ml.classification import DecisionTreeClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "aqAG3-Mc6glx"
      },
      "outputs": [],
      "source": [
        "dt = DecisionTreeClassifier(featuresCol='Features', labelCol='target')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "b5MoY_w46gly"
      },
      "outputs": [],
      "source": [
        "dt = dt.fit(train_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Prediction**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "yyk6Ost36gly"
      },
      "outputs": [],
      "source": [
        "\n",
        "y_pred = dt.transform(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLNLJhI16gly",
        "outputId": "17fe2eb1-0f64-424f-cd3c-1913808eaf31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+------+-------------+--------------------+----------+\n",
            "|            Features|target|rawPrediction|         probability|prediction|\n",
            "+--------------------+------+-------------+--------------------+----------+\n",
            "|[8.671,14.45,54.4...|     1|  [1.0,243.0]|[0.00409836065573...|       1.0|\n",
            "|[8.726,15.83,55.8...|     1|  [1.0,243.0]|[0.00409836065573...|       1.0|\n",
            "|[9.465,21.01,60.1...|     1|  [1.0,243.0]|[0.00409836065573...|       1.0|\n",
            "|[9.876,19.4,63.95...|     1|  [1.0,243.0]|[0.00409836065573...|       1.0|\n",
            "|[10.18,17.53,65.1...|     1|  [1.0,243.0]|[0.00409836065573...|       1.0|\n",
            "|[10.48,19.86,66.7...|     1|  [1.0,243.0]|[0.00409836065573...|       1.0|\n",
            "|[10.49,19.29,67.4...|     1|  [1.0,243.0]|[0.00409836065573...|       1.0|\n",
            "|[10.51,20.19,68.6...|     1|  [1.0,243.0]|[0.00409836065573...|       1.0|\n",
            "|[10.65,25.22,68.0...|     1|  [1.0,243.0]|[0.00409836065573...|       1.0|\n",
            "|[10.71,20.39,69.5...|     1|  [1.0,243.0]|[0.00409836065573...|       1.0|\n",
            "|[10.75,14.97,68.2...|     1|  [1.0,243.0]|[0.00409836065573...|       1.0|\n",
            "|[11.41,10.82,73.3...|     1|  [1.0,243.0]|[0.00409836065573...|       1.0|\n",
            "|[11.52,14.93,73.8...|     1|  [1.0,243.0]|[0.00409836065573...|       1.0|\n",
            "|[11.71,16.67,74.7...|     1|  [1.0,243.0]|[0.00409836065573...|       1.0|\n",
            "|[11.76,21.6,74.72...|     1|  [1.0,243.0]|[0.00409836065573...|       1.0|\n",
            "|[11.89,18.35,77.3...|     1|  [1.0,243.0]|[0.00409836065573...|       1.0|\n",
            "|[12.3,15.9,78.83,...|     1|  [1.0,243.0]|[0.00409836065573...|       1.0|\n",
            "|[12.36,18.54,79.0...|     1|  [1.0,243.0]|[0.00409836065573...|       1.0|\n",
            "|[12.42,15.04,78.6...|     1|  [1.0,243.0]|[0.00409836065573...|       1.0|\n",
            "|[12.77,22.47,81.7...|     0|  [1.0,243.0]|[0.00409836065573...|       1.0|\n",
            "+--------------------+------+-------------+--------------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "y_pred.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Confusion Matrix**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aP0v4QDgrP8Q",
        "outputId": "f92d9bbb-6e11-4607-dfc2-bec7aecd4ae6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+----------+-----+\n",
            "|target|prediction|count|\n",
            "+------+----------+-----+\n",
            "|     1|       0.0|    7|\n",
            "|     0|       1.0|    3|\n",
            "|     0|       0.0|   38|\n",
            "|     1|       1.0|   64|\n",
            "+------+----------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "y_pred.groupBy('target', 'prediction').count().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57T0b7gOw_t9",
        "outputId": "386cdfce-52e8-4eb3-becb-ffe37a452d2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[38  3]\n",
            " [ 7 64]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "pred=y_pred.select(\"prediction\").collect()\n",
        "orig=y_pred.select(\"target\").collect()\n",
        "print(confusion_matrix(orig, pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBMExg8BDIi2"
      },
      "source": [
        "(f1|accuracy|weightedPrecision|weightedRecall|weightedTruePositiveRate| weightedFalsePositiveRate|weightedFMeasure|truePositiveRateByLabel| falsePositiveRateByLabel|precisionByLabel|recallByLabel|fMeasureByLabel| logLoss|hammingLoss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "1z9f4RbN_2NU"
      },
      "outputs": [],
      "source": [
        "\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "9keKjBLvxq_c"
      },
      "outputs": [],
      "source": [
        "evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol='target', predictionCol='prediction')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNbJEY7A_3NG",
        "outputId": "d1d134e9-1c09-494f-c2a3-dfeaf00de794"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.9114967018152054"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "accuracy = evaluator.evaluate(y_pred)\n",
        "accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Close connection to Spark**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "T5fn3z4r6gly"
      },
      "outputs": [],
      "source": [
        "\n",
        "spark.stop()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
